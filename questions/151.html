<!DOCTYPE html>
<html>
  <head>
        <title>Training using half-precision floating point (fp16) can be up to 3x faster. When training with fp16, are all calculations done using half-precision floats?</title>
        <meta name="description" content="No, we cannot do that, because fp16 is not accurate enough, its hard to get good gradients with fp16 (things often round off to zero). Instead, we do the h...">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="yKtxJMzLKmIvBASpMahOIwooyDjPctEgXOYZgv32OgVxoAbMbb3qtTk0JM+IR12sojUNbBMURjYlb1023Vo81Q==" />
    

    <link rel="stylesheet" media="all" href="../packs/css/application-5184725d.css" data-turbolinks-track="reload" />
    <script src="../packs/js/application-d5fac83c9fc8d3ee256f.js" data-turbolinks-track="reload"></script>
    <link href="https://storage.googleapis.com/aiquizzes/static/favicon.ico" rel="shortcut icon" type="image/x-icon" />
  </head>
  <body class="text-gray-900 antialiased">
    <header class="bg-gray-100">
      <div class="sm:flex sm:justify-between sm:items-center max-w-5xl m-auto">
        <div class="flex sm:block items-center justify-between px-2 py-1">
          <a class="no-underline" href="../index.html"><div class="font-bold text-2xl mt-0"><span class="text-orange-500">ai</span>quizzes
            </div></a>

          <div class="sm:hidden">
            <button id="nav_button" type="button" class="block text-gray-600 focus:text-gray-900 hover:text-gray-900 focus:outline-none">
              <svg class="h-6 w-6 fill-current" viewBox="0 0 24 24">
                <path id="nav_hamburger" fill-rule="evenodd" d="M4 5h16a1 1 0 0 1 0 2H4a1 1 0 1 1 0-2zm0 6h16a1 1 0 0 1 0 2H4a1 1 0 0 1 0-2zm0 6h16a1 1 0 0 1 0 2H4a1 1 0 0 1 0-2z"/>
                <path id="nav_x" class="hidden" fill-rule="evenodd" d="M18.278 16.864a1 1 0 0 1-1.414 1.414l-4.829-4.828-4.828 4.828a1 1 0 0 1-1.414-1.414l4.828-4.829-4.828-4.828a1 1 0 0 1 1.414-1.414l4.829 4.828 4.828-4.828a1 1 0 1 1 1.414 1.414l-4.828 4.829 4.828 4.828z"/>
              </svg>
            </button>
          </div>
        </div>
        <div class="px-2 pb-2 hidden sm:flex" id="nav_links">
          <a class="block px-2 py-1 font-semibold hover:bg-gray-300 rounded sm:mt-2 sm:ml-2 no-underline" href="../knowledgebase.html">Knowledgebase</a>
        </div>
      </div>
    </header>

    <div class="max-w-xl p-8 mx-auto font-serif text-lg">
      <div class="flex justify-between">
  <div class="uppercase text-xs font-semibold text-gray-600 tracking-wide font-sans mt-0">
    Question 4/13 fast.ai v3 lecture 12
  </div>
</div>


<h1 class="shadow-md p-3">
  Training using half-precision floating point (fp16) can be up to 3x faster. When training with fp16, are all calculations done using half-precision floats?
</h1>

<p class="uppercase text-xs font-semibold text-gray-600 tracking-wide font-sans mt-6">Answer</p>
<article class="shadow-md p-3">
  No, we cannot do that, because fp16 is not accurate enough, its hard to get good gradients with fp16 (things often round off to zero). Instead, we do the heavy lifting in fp16 - the forward pass and the backward pass. Everywhere else, we convert tensors to fp32 and perform the operations in full precision (calculating the loss, subtracting gradients). <a href="../gcp_images/mixed_precision_training.png"><img src="../gcp_images/mixed_precision_training.png"></a><br> 
</article>


  <p class="uppercase text-xs font-semibold text-gray-600 tracking-wide font-sans mt-6">Relevant part of lecture</p>
  <div class="embed-responsive aspect-ratio-16/9 flex justify-center mt-1 shadow-md">
    <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/vnOpEwmtFJ8?start=1350" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

  </div>

  <p class="uppercase text-xs font-semibold text-gray-600 tracking-wide font-sans mt-6">supplementary material</p>
    <div class="text-sm shadow-md p-3">
      <a href="https://youtu.be/vnOpEwmtFJ8?t=1563">Later in the lecture</a>: Fp16 sometime trains a little bit better than fp32 - maybe it has some regularazing effect? Generally, the results are very close together
    </div>


    </div>
  </body>
</html>
